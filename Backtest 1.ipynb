{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Alpha Backtest\n",
    "## Considerations in the backtest\n",
    "In the backtest, the followings will be need to be additionally considered:\n",
    "### Time delay in live trading\n",
    "Realistically, there will be a two days time delay in live trading as PnL is reported for the date of returns.\n",
    " - T+0 EOD Factors -> Decide trading\n",
    " - T+1 Trade execution on T+1 Price/Market\n",
    " - T+2 Calculate returns on EOD Price\n",
    "The negative impact of the continuous trade executions on the return date [T+2]  based on the trading decisions on T+1 will be factored in to Transaction Cost.\n",
    "\n",
    "### Remove look ahead bias\n",
    " - Universe should be selected on each day to avoid survivership bias.\n",
    " - Should use only the data available to each day. Do not calculate the aplha vector by an estimator trained using future date. Continuously training the estimator using the latest data available for each day.\n",
    "\n",
    "### Trading Cost and Liquidity\n",
    "Construct the universe of stocks as only those companies\n",
    " - that have a large liquidity which has capacity to absorb all trades with less price impact OR\n",
    " - that are in the previous¥ day's holdings\n",
    "Explicit cost such as commissions are far smaller than market impact.\n",
    "Define the transaction cost, or slippage, as to multiply the price change due to market impact by the amount of dollars traded.\n",
    "\n",
    "Here, use linear impact model because the slope is not available in public.\n",
    "Good estimate according the past researches is, each 1% of Average Daily Volume Traded moves the price by 10 basis point.\n",
    "\n",
    "$$\n",
    "\\mbox{tcost_{i,t}} = \\% \\Delta \\mbox{price}_{i,t} \\times \\mbox{trade}_{i,t} = (0.1 \\times (h_{i,t} - h_{i,t-1}) \\div \\mbox{ADV}_{i,t}) \\times (h_{i,t} - h_{i,t-1})\n",
    "$$\n",
    "\n",
    "That is,\n",
    "$$\n",
    "\\mbox{tcost}_{i,t} = \\sum_i^{N} \\lambda_{i,t} (h_{i,t} - h_{i,t-1})^2\n",
    "$$  \n",
    "where\n",
    "$$\n",
    "\\lambda_{i,t} = \\frac{1}{10\\times \\mbox{ADV}_{i,t}}\n",
    "$$\n",
    "\n",
    "When ADV is missing or zero, consider it to be a small number like 10,000 and highly illiquid.\n",
    "\n",
    "Then, total transaction costs will be:\n",
    "$$\n",
    "\\mbox{tcost} = \\sum_i^{N} \\lambda_{i} (h_{i,t} - h_{i,t-1})^2\n",
    "$$\n",
    "\n",
    "### Winsorize\n",
    "Clip the data to avoid extremely positive or negative values to handle noise causing unusually large positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data available for backtest\n",
    "This means, the data available for back test would be around 2 years, 2014 and 2015 because\n",
    " - Original data: 5 years from Jan 2011 to Jan 2016\n",
    " - Factor data: 4 years from Jan 2012 to Jan 2016\n",
    " - Weekly return: The last week cannot be used, so from Jan 2012 to Dec 2015\n",
    " - Keep rolling window of 2 years for training AI Alpha, resulting in the range from Jan 2014 to Dec 2015\n",
    "\n",
    "In addition, covariance matrix should be calculated on each day as the universe changes on daily basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://files.pythonhosted.org/\n",
      "Requirement already satisfied: alphalens==0.3.6 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (0.3.6)\n",
      "Requirement already satisfied: graphviz==0.10.1 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.10.1)\n",
      "Requirement already satisfied: numpy==1.16.2 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.16.2)\n",
      "Requirement already satisfied: pandas==0.22.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.22.0)\n",
      "Requirement already satisfied: matplotlib==2.1.2 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil==2.6.1 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (2.6.1)\n",
      "Requirement already satisfied: pytz==2017.3 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (2017.3)\n",
      "Requirement already satisfied: scipy==1.0.1 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn==0.19.1 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (0.19.1)\n",
      "Requirement already satisfied: six==1.11.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 10)) (1.11.0)\n",
      "Requirement already satisfied: tables==3.3.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 11)) (3.3.0)\n",
      "Requirement already satisfied: tqdm==4.19.5 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 12)) (4.19.5)\n",
      "Requirement already satisfied: zipline===1.3.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: colour==0.1.5 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 15)) (0.1.5)\n",
      "Requirement already satisfied: cvxpy==1.0.3 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 16)) (1.0.3)\n",
      "Requirement already satisfied: cycler==0.10.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 17)) (0.10.0)\n",
      "Requirement already satisfied: plotly==2.2.3 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 18)) (2.2.3)\n",
      "Requirement already satisfied: pyparsing==2.2.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 19)) (2.2.0)\n",
      "Requirement already satisfied: requests==2.18.4 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 20)) (2.18.4)\n",
      "Requirement already satisfied: patsy==0.5.1 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 22)) (0.5.1)\n",
      "Requirement already satisfied: statsmodels==0.9.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from -r requirements.txt (line 23)) (0.9.0)\n",
      "Requirement already satisfied: IPython>=3.2.3 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from alphalens==0.3.6->-r requirements.txt (line 1)) (7.16.1)\n",
      "Requirement already satisfied: seaborn>=0.6.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from alphalens==0.3.6->-r requirements.txt (line 1)) (0.10.1)\n",
      "Requirement already satisfied: numexpr>=2.5.2 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from tables==3.3.0->-r requirements.txt (line 11)) (2.7.1)\n",
      "Requirement already satisfied: bottleneck>=1.0.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (1.3.2)\n",
      "Requirement already satisfied: empyrical>=0.5.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (0.5.3)\n",
      "Requirement already satisfied: contextlib2>=0.4.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (0.6.0.post1)\n",
      "Requirement already satisfied: alembic>=0.7.7 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (1.4.2)\n",
      "Requirement already satisfied: Logbook>=0.12.5 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (1.5.3)\n",
      "Requirement already satisfied: Cython>=0.25.2 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (0.29.21)\n",
      "Requirement already satisfied: lru-dict>=1.1.4 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (1.1.6)\n",
      "Requirement already satisfied: requests-file>=1.4.1 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (1.5.1)\n",
      "Requirement already satisfied: click>=4.0.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (7.1.2)\n",
      "Requirement already satisfied: Mako>=1.0.1 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (1.1.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.0.8 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (1.3.18)\n",
      "Requirement already satisfied: intervaltree>=2.1.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (3.0.2)\n",
      "Requirement already satisfied: bcolz<1,>=0.12.1 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: pip>=7.1.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (20.1.1)\n",
      "Requirement already satisfied: setuptools>18.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (47.3.1.post20200622)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (1.1.1)\n",
      "Requirement already satisfied: cyordereddict>=0.2.2 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (1.0.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (0.10.0)\n",
      "Requirement already satisfied: pandas-datareader>=0.2.1 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (0.5.0)\n",
      "Requirement already satisfied: trading-calendars>=1.0.1 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (1.11.8)\n",
      "Requirement already satisfied: multipledispatch>=0.4.8 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (0.6.0)\n",
      "Requirement already satisfied: networkx<2.0,>=1.9.1 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (1.11)\n",
      "Requirement already satisfied: sortedcontainers>=1.4.4 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (2.2.2)\n",
      "Requirement already satisfied: decorator>=4.0.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from zipline===1.3.0->-r requirements.txt (line 13)) (4.4.2)\n",
      "Requirement already satisfied: scs>=1.1.3 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from cvxpy==1.0.3->-r requirements.txt (line 16)) (2.1.2)\n",
      "Requirement already satisfied: multiprocess in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from cvxpy==1.0.3->-r requirements.txt (line 16)) (0.70.10)\n",
      "Requirement already satisfied: osqp in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from cvxpy==1.0.3->-r requirements.txt (line 16)) (0.6.1)\n",
      "Requirement already satisfied: ecos>=2 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from cvxpy==1.0.3->-r requirements.txt (line 16)) (2.0.7.post1)\n",
      "Requirement already satisfied: fastcache in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from cvxpy==1.0.3->-r requirements.txt (line 16)) (1.1.0)\n",
      "Requirement already satisfied: nbformat>=4.2 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from plotly==2.2.3->-r requirements.txt (line 18)) (5.0.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 20)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 20)) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 20)) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 20)) (1.22)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.6->-r requirements.txt (line 1)) (0.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.6->-r requirements.txt (line 1)) (0.17.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.6->-r requirements.txt (line 1)) (4.3.3)\n",
      "Requirement already satisfied: backcall in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.6->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.6->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.6->-r requirements.txt (line 1)) (3.0.5)\n",
      "Requirement already satisfied: pickleshare in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.6->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: pygments in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.6->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: python-editor>=0.3 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from alembic>=0.7.7->zipline===1.3.0->-r requirements.txt (line 13)) (1.0.4)\n",
      "Requirement already satisfied: requests-ftp in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from pandas-datareader>=0.2.1->zipline===1.3.0->-r requirements.txt (line 13)) (0.3.1)\n",
      "Requirement already satisfied: dill>=0.3.2 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from multiprocess->cvxpy==1.0.3->-r requirements.txt (line 16)) (0.3.2)\n",
      "Requirement already satisfied: future in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from osqp->cvxpy==1.0.3->-r requirements.txt (line 16)) (0.18.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 18)) (3.2.0)\n",
      "Requirement already satisfied: ipython-genutils in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 18)) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 18)) (4.6.3)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from jedi>=0.10->IPython>=3.2.3->alphalens==0.3.6->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->IPython>=3.2.3->alphalens==0.3.6->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython>=3.2.3->alphalens==0.3.6->-r requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 18)) (19.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 18)) (1.7.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 18)) (0.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 18)) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuki/miniconda3/envs/ai-alpha/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "\n",
    "import cvxpy as cvx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "import importlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "### Data Bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from zipline.data import bundles\n",
    "\n",
    "os.environ['ZIPLINE_ROOT'] = os.path.join(os.getcwd(), 'data', 'zipline')\n",
    "\n",
    "ingest_func = bundles.csvdir.csvdir_equities(['daily'], helper.EOD_BUNDLE_NAME)\n",
    "bundles.register(helper.EOD_BUNDLE_NAME, ingest_func)\n",
    "\n",
    "print('Data Registered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universe\n",
    "Since the liquidity matters, choose top 500 stocks in terms of average dollar volume traded among highly developed market - US Equity.\n",
    "Bid-ask spead for these equities are 2-5bp, as oppose to 20-30bp for less liquid stocks. To target 400bp annual return from daily trading, more than 10bp for a single trade is not acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.factors import AverageDollarVolume\n",
    "from zipline.utils.calendars import get_calendar\n",
    "\n",
    "universe = AverageDollarVolume(window_length=120).top(500) \n",
    "trading_calendar = get_calendar('NYSE') \n",
    "bundle_data = bundles.load(helper.EOD_BUNDLE_NAME)\n",
    "engine = helper.build_pipeline_engine(bundle_data, trading_calendar)\n",
    "sector = helper.Sector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.data.data_portal import DataPortal\n",
    "\n",
    "data_portal = DataPortal(\n",
    "    bundle_data.asset_finder,\n",
    "    trading_calendar=trading_calendar,\n",
    "    first_trading_day=bundle_data.equity_daily_bar_reader.first_trading_day,\n",
    "    equity_minute_reader=None,\n",
    "    equity_daily_reader=bundle_data.equity_daily_bar_reader,\n",
    "    adjustment_reader=bundle_data.adjustment_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2014-01-17 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-01-21 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-01-22 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-01-23 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-01-24 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-01-27 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-01-28 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-01-29 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-01-30 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-01-31 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-03 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-04 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-05 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-06 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-07 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-10 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-11 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-12 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-13 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-14 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-18 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-19 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-20 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-21 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-24 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-25 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-26 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-27 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-02-28 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-03 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-04 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-05 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-06 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-07 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-10 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-11 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-12 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-13 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-14 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-17 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-18 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-19 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-20 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-21 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-24 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-25 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-26 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-27 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-28 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-03-31 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-01 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-02 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-03 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-04 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-07 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-08 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-09 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-10 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-11 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-14 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-15 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-16 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-17 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-21 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-22 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-23 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-24 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-25 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-28 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-29 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-04-30 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-01 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-02 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-05 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-06 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-07 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-08 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-09 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-12 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-13 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-14 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-15 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-16 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-19 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-20 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-21 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-22 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-23 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-27 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-28 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-29 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-05-30 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-02 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-03 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-04 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-05 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-06 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-09 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-10 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-11 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-12 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-13 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-16 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-17 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-18 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-19 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-20 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-23 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-24 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-25 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-26 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-27 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-06-30 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-01 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-02 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-03 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-07 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-08 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-09 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-10 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-11 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-14 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-15 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-16 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-17 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-18 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-21 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-22 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-23 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-24 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-25 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-28 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-29 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-30 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-07-31 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-01 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-04 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-05 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-06 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-07 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-08 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-11 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-12 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-13 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-14 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-15 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-18 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-19 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-20 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-21 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-22 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-25 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-26 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-27 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-28 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-08-29 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-02 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-03 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-04 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-05 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-08 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-09 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-10 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-11 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-12 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-15 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-16 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-17 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-18 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-19 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-22 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-23 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-24 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-25 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-26 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-29 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-09-30 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-01 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-02 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-03 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-06 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-07 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-08 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-09 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-10 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-13 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-14 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-15 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-16 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-17 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-20 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-21 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-22 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-23 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-24 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-27 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-28 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-29 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-30 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-10-31 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-03 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-04 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-05 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-06 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-07 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-10 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-11 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-12 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-13 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-14 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-17 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-18 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-19 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-20 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-21 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-24 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-25 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-26 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-11-28 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-01 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-02 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-03 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-04 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-05 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-08 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-09 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-10 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-11 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-12 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-15 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-16 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-17 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-18 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-19 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-22 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-23 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-24 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-26 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-29 00:00:00+0000', tz='UTC', freq='C'),\n",
       " Timestamp('2014-12-30 00:00:00+0000', tz='UTC', freq='C')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Factor start and end date\n",
    "start_date = pd.Timestamp('2014-01-15', tz='UTC', freq='C')\n",
    "end_date = pd.Timestamp('2014-12-31', tz='UTC', freq='C')\n",
    "\n",
    "start_loc = trading_calendar.closes.index.get_loc(start_date)\n",
    "end_loc = trading_calendar.closes.index.get_loc(end_date)\n",
    "\n",
    "backtest_dates = [pd.Timestamp(x.strftime('%Y-%m-%d'), tz='UTC', freq='C') for x in trading_calendar.closes[start_loc+2:end_loc]]\n",
    "backtest_dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_lookup = pd.read_csv('./data/sector/labels.csv', index_col='Sector_i')['Sector'].to_dict()\n",
    "sector_columns = []\n",
    "for sector_i, sector_name in sector_lookup.items():\n",
    "    secotr_column = 'sector_{}'.format(sector_name)\n",
    "    sector_columns.append(secotr_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'Mean_Reversion_Sector_Neutral_Smoothed', 'Momentum_1YR',\n",
    "    'Overnight_Sentiment_Smoothed', 'adv_120d', 'adv_20d',\n",
    "    'dispersion_120d', 'dispersion_20d', 'market_vol_120d',\n",
    "    'market_vol_20d', 'volatility_20d',\n",
    "    'is_Janaury', 'is_December', 'weekday',\n",
    "    'month_end', 'month_start', 'qtr_end', 'qtr_start'] + sector_columns\n",
    "target_label = 'target'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 10\n",
    "n_stocks = 500\n",
    "clf_random_state = 42\n",
    "\n",
    "clf_parameters = {\n",
    "    'n_estimators': 250,\n",
    "    'criterion': 'entropy',\n",
    "    'min_samples_leaf': n_stocks * n_days,\n",
    "    'oob_score': True,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': clf_random_state}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_optimal_portfolio(factor_date, return_date, previous, universe):\n",
    "    '''Returns the optimal portfolio, risk and alpha exposures and the total transaction cost'''\n",
    "    factor_date_loc = trading_calendar.closes.index.get_loc(factor_date)\n",
    "    factor_start_date = pd.Timestamp(trading_calendar.closes[factor_date_loc-252*3].strftime('%Y-%m-%d'), tz='UTC', freq='C')\n",
    "    print(\"Factor Start Date: \", factor_start_date.strftime('%Y%m%d'))\n",
    "    alpha_factors = helper.get_alpha_factors(universe, sector, engine, factor_start_date, factor_date)\n",
    "    alpha_factors_today = alpha_factors.loc[alpha_factors.index.get_level_values(0) == factor_date]\n",
    "    df = alpha_factors_today.reset_index(level=0, drop=True)\n",
    "    df = df.merge(previous, left_index=True, right_index=True, how='left')\n",
    "    df = helper.clean_nas(df)\n",
    "\n",
    "    universe_tickers = helper.get_universe_tickers(factor_date, universe, engine)\n",
    "    \n",
    "    three_year_pricing = helper.get_pricing(\n",
    "        data_portal,\n",
    "        trading_calendar,\n",
    "        universe_tickers,\n",
    "        factor_start_date,\n",
    "        factor_date)\n",
    "\n",
    "    three_year_returns = three_year_pricing.pct_change()[1:].fillna(0)\n",
    "  \n",
    "    risk_model = helper.get_risk_factors(three_year_returns)\n",
    "    \n",
    "    today_pricing = three_year_pricing.loc[three_year_pricing.index.get_level_values(0) == factor_date]\n",
    "\n",
    "    today_returns = three_year_returns.loc[three_year_returns.index.get_level_values(0) == factor_date]\n",
    "    today_returns = today_returns.reset_index(level=0, drop=True)\n",
    "\n",
    "    result = helper.train_model(alpha_factors, features, target_label, clf_parameters)\n",
    "    alpha_vector = helper.get_alpha_vector(result[0], df, features, target_label)\n",
    "  \n",
    "    Lambda = helper.get_lambda(df)\n",
    "\n",
    "    h_star = helper.OptimalHoldingsStrictFactor(\n",
    "        weights_max=0.05,\n",
    "        weights_min=-0.05,\n",
    "        risk_cap=0.0015,\n",
    "        factor_max=0.035,\n",
    "        factor_min=-0.035).find(alpha_vector,\n",
    "                                risk_model['factor_betas'], \n",
    "                                risk_model['factor_cov_matrix'], \n",
    "                                risk_model['idiosyncratic_var_vector'])\n",
    "    h_star = h_star * 1e6 \n",
    "    \n",
    "    port_risk = helper.predict_portfolio_risk(\n",
    "        risk_model['factor_betas'],\n",
    "        risk_model['factor_cov_matrix'],\n",
    "        risk_model['idiosyncratic_var_matrix'],\n",
    "        h_star)\n",
    "        \n",
    "    \n",
    "    risk_exposures = helper.get_risk_exposures(risk_model['factor_betas'], h_star)\n",
    "    alpha_exposure = helper.get_alpha_exposures(alpha_vector, h_star)\n",
    "    total_transaction_costs = helper.get_total_transaction_costs(previous, h_star, Lambda)\n",
    "    \n",
    "    alpha_vector_with_date = alpha_vector.set_index(pd.Index([factor_date]*alpha_vector.shape[0], name='date'), append=True)\n",
    "    alpha_vector_with_date = alpha_vector_with_date.swaplevel()\n",
    "    \n",
    "    return {\n",
    "        \"opt.portfolio\" : h_star,\n",
    "        \"portfolio.risk\": port_risk,\n",
    "        \"risk.return\" : risk_model['factor_returns'], \n",
    "        \"risk.exposures\" : risk_exposures, \n",
    "        \"alpha.exposures\" : alpha_exposure,\n",
    "        \"total.cost\" : total_transaction_costs,\n",
    "        \"alpha.vector\": alpha_vector_with_date,\n",
    "        \"today.returns\": today_returns}\n",
    "\n",
    "def build_tradelist(prev_holdings, opt_result):\n",
    "    '''Returns the trades on the day'''\n",
    "    tmp = prev_holdings.merge(opt_result['opt.portfolio'], how='outer', left_index=True, right_index=True)\n",
    "    tmp['h.opt.previous'] = np.nan_to_num(tmp['h.opt.previous'])\n",
    "    tmp['h.opt'] = np.nan_to_num(tmp['h.opt'])\n",
    "    return tmp\n",
    "\n",
    "def convert_to_previous(result): \n",
    "    '''Save optimal holdings as previous optimal holdings'''\n",
    "    prev = result['opt.portfolio']\n",
    "    prev = prev.rename(index=str, columns={\"h.opt\": \"h.opt.previous\"}, copy=True, inplace=False)\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimizing Portfolio:   0%|          | 0/240 [00:00<?, ?day/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Date:  20140117\n",
      "Factor Date 20140115\n",
      "Factor Start Date:  20110112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimizing Portfolio:   0%|          | 1/240 [00:44<2:56:02, 44.19s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Date:  20140121\n",
      "Factor Date 20140116\n",
      "Factor Start Date:  20110113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimizing Portfolio:   1%|          | 2/240 [01:24<2:47:04, 42.12s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Date:  20140122\n",
      "Factor Date 20140117\n",
      "Factor Start Date:  20110114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimizing Portfolio:   1%|▏         | 3/240 [02:05<2:45:29, 41.90s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Date:  20140123\n",
      "Factor Date 20140121\n",
      "Factor Start Date:  20110118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimizing Portfolio:   2%|▏         | 4/240 [02:52<2:49:09, 43.01s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Date:  20140124\n",
      "Factor Date 20140122\n",
      "Factor Start Date:  20110119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimizing Portfolio:   2%|▏         | 5/240 [03:37<2:50:25, 43.51s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Date:  20140127\n",
      "Factor Date 20140123\n",
      "Factor Start Date:  20110120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimizing Portfolio:   2%|▎         | 6/240 [04:27<2:53:41, 44.54s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Date:  20140128\n",
      "Factor Date 20140124\n",
      "Factor Start Date:  20110121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimizing Portfolio:   3%|▎         | 7/240 [05:12<2:53:20, 44.64s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Date:  20140129\n",
      "Factor Date 20140127\n",
      "Factor Start Date:  20110124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimizing Portfolio:   3%|▎         | 8/240 [05:58<2:53:09, 44.78s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Date:  20140130\n",
      "Factor Date 20140128\n",
      "Factor Start Date:  20110125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimizing Portfolio:   4%|▍         | 9/240 [06:52<2:56:17, 45.79s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Date:  20140131\n",
      "Factor Date 20140129\n",
      "Factor Start Date:  20110126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimizing Portfolio:   4%|▍         | 10/240 [07:40<2:56:31, 46.05s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Date:  20140203\n",
      "Factor Date 20140130\n",
      "Factor Start Date:  20110127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimizing Portfolio:   5%|▍         | 11/240 [08:28<2:56:32, 46.25s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Date:  20140204\n",
      "Factor Date 20140131\n",
      "Factor Start Date:  20110128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Optimizing Portfolio:   5%|▌         | 12/240 [09:22<2:58:03, 46.86s/day]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Date:  20140205\n",
      "Factor Date 20140203\n",
      "Factor Start Date:  20110131\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(helper)\n",
    "result = {}\n",
    "trades = {}\n",
    "portfolio = {}\n",
    "\n",
    "universe_tickers = helper.get_universe_tickers(backtest_dates[0], universe, engine)\n",
    "previous_holdings = pd.DataFrame(data = {\"h.opt.previous\" : np.array(0)}, index=universe_tickers)\n",
    "\n",
    "\n",
    "for dt in tqdm(backtest_dates, desc='Optimizing Portfolio', unit='day'):\n",
    "    date = dt.strftime('%Y%m%d')\n",
    "    return_date = dt\n",
    "    print(\"Return Date: \", return_date.strftime('%Y%m%d'))\n",
    "    return_date_loc = trading_calendar.closes.index.get_loc(return_date)\n",
    "    factor_date = pd.Timestamp(trading_calendar.closes[return_date_loc-2].strftime('%Y-%m-%d'), tz='UTC', freq='C')\n",
    "    print(\"Factor Date\", factor_date.strftime('%Y%m%d'))\n",
    "    \n",
    "    result[date] = form_optimal_portfolio(factor_date, return_date, previous_holdings, universe)\n",
    "    trades[date] = build_tradelist(previous_holdings, result[date])\n",
    "    portfolio[date] = result[date]\n",
    "    previous_holdings = convert_to_previous(result[date])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PnL Attribution\n",
    "Profit and Loss is the aggregate realized daily returns of the assets, weighted by the optimal portfolio holdings chosen, and summed up to get the portfolio's profit and loss.\n",
    "\n",
    "The PnL attributed to the alpha factors equals the factor returns times factor exposures for the alpha factors.  \n",
    "\n",
    "$$\n",
    "\\mbox{PnL}_{alpha}= f \\times b_{alpha}\n",
    "$$\n",
    "\n",
    "Similarly, the PnL attributed to the risk factors equals the factor returns times factor exposures of the risk factors.\n",
    "\n",
    "$$\n",
    "\\mbox{PnL}_{risk} = f \\times b_{risk}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def partial_dot_product(v, w):\n",
    "    '''Return sum of two Pandas Series for common indices.'''\n",
    "    common = v.index.intersection(w.index)\n",
    "    return np.sum(v[common] * w[common])\n",
    "\n",
    "def build_pnl_attribution():\n",
    "    new=1\n",
    "    for dt in backtest_dates:\n",
    "        date = dt.strftime('%Y%m%d')\n",
    "        p = portfolio[date]\n",
    "        if new:\n",
    "            alpha_vector = p['alpha.vector']\n",
    "            new = 0\n",
    "        else:\n",
    "            alpha_vector = pd.concat([alpha_vector, p['alpha.vector']])\n",
    "    \n",
    "    all_assets = alpha_vector.index.levels[1].values.tolist()\n",
    "    all_pricing = helper.get_pricing(\n",
    "        data_portal,\n",
    "        trading_calendar,\n",
    "        all_assets,\n",
    "        start_date,\n",
    "        end_date)\n",
    "    # print(alpha_vector)\n",
    "    factor_data = helper.build_factor_data(alpha_vector, all_pricing)\n",
    "    \n",
    "    alpha_returns = helper.get_factor_returns(factor_data)\n",
    "    qr_alpha_returns = helper.get_qr_factor_returns(factor_data)\n",
    "    sharpe_ratio = helper.sharpe_ratio(alpha_returns)\n",
    "    \n",
    "    df = pd.DataFrame(index = backtest_dates)\n",
    "    \n",
    "    for dt in tqdm(backtest_dates[:-3]):\n",
    "        date = dt.strftime('%Y%m%d')\n",
    "\n",
    "        p = portfolio[date]\n",
    "        \n",
    "        alpha_returns_today = alpha_returns.loc[alpha_returns.index.get_level_values(0) == dt]\n",
    "        alpha_returns_today = alpha_returns_today.reset_index(level=0, drop=True)\n",
    "\n",
    "        mf = p['opt.portfolio'].merge(p['today.returns'].T, how='left', left_index=True, right_index=True)\n",
    "        mf['today.returns'] = helper.wins(mf[0], -0.5, 0.5)\n",
    "        df.at[dt,\"daily.pnl\"] = np.sum(mf['h.opt'] * mf['today.returns'])\n",
    "        \n",
    "        # df.at[dt,\"attribution.alpha.pnl\"] = partial_dot_product(alpha_returns_today, p['alpha.exposures'])\n",
    "        df.at[dt,\"attribution.alpha.pnl\"] = alpha_returns_today.values[0][0] * p['alpha.exposures'].values[0][0]\n",
    "        #df.at[dt,\"attribution.risk.pnl\"] = partial_dot_product(p['risk.return'].iloc[-1], p['risk.exposures']) \n",
    "        df.at[dt,\"attribution.risk.pnl\"] = np.dot(np.asarray(p['risk.return'].iloc[-1]), np.asarray(p['risk.exposures']))[0]\n",
    "        df.at[dt,\"attribution.cost\"] = p['total.cost'] * -1\n",
    "        \n",
    "    return df \n",
    "\n",
    "attr = build_pnl_attribution()\n",
    "attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(15,7))\n",
    "for column in attr.columns:\n",
    "        plt.plot(attr[column].cumsum(), label=column)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PnL Attribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_portfolio_characteristics(): \n",
    "    df = pd.DataFrame(index = backtest_dates)\n",
    "    \n",
    "    for dt in backtest_dates:\n",
    "        date = dt.strftime('%Y%m%d')\n",
    "  \n",
    "        p = portfolio[date]\n",
    "        tradelist = trades[date]\n",
    "        h = p['opt.portfolio']['h.opt']\n",
    "        \n",
    "        df.at[dt,\"long\"] = np.sum(h[h>0])\n",
    "        df.at[dt,\"short\"] = np.sum(h[h<0])\n",
    "        df.at[dt,\"net\"] = np.sum(h)\n",
    "        df.at[dt,\"gmv\"] = np.sum(np.abs(h))\n",
    "        df.at[dt,\"traded\"] = np.sum(abs(tradelist['h.opt'] - tradelist['h.opt.previous']))\n",
    "        \n",
    "    return df\n",
    "\n",
    "pchar = build_portfolio_characteristics()\n",
    "pchar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(15,7))\n",
    "for column in pchar.columns:\n",
    "        plt.plot(pchar[column], label=column)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Portfolio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further consideration\n",
    "### Objective Function\n",
    "It is not efficient to optimize the weight by imposing constraints. Equality constraints may result in no optima, whereas unequality constraints all tend to end up local optima or partial optimization.\n",
    "The objective function to minimize can be given as:\n",
    "\n",
    "$$\\begin{align} f(\\mathbf{h}) &= \\mbox{factor risk (Risk aversion} \\times \\mbox{Common Risk)} \\\\\n",
    " &+\\mbox{idiosyncratic risk (Risk Aversion} \\times \\mbox{Specific Risk)} \\\\\n",
    " &-\\mbox{expected portfolio return} \\\\\n",
    " &+ \\mbox{transaction costs} \\\\\n",
    " &= \\frac{1}{2}\\kappa \\mathbf{h}_t^T\\mathbf{Q}^T\\mathbf{Q}\\mathbf{h}_t + \\frac{1}{2} \\kappa \\mathbf{h}_t^T \\mathbf{S} \\mathbf{h}_t - \\mathbf{\\alpha}^T \\mathbf{h}_t + (\\mathbf{h}_{t} - \\mathbf{h}_{t-1})^T \\mathbf{\\Lambda} (\\mathbf{h}_{t} - \\mathbf{h}_{t-1}) \\end{align}\n",
    "$$\n",
    "\n",
    "where $\\textbf{B}$ is $N$ by $K$ matrix, $\\textbf{F}$ is $K$ by $K$ matrix and $\\textbf{B}^T$ is $K$ by $N$ matrix, resulting in $\\textbf{BFB}^T$ as a huge $N \\times N$ matrix (e.g. 2,000 assets $\\times$ 2,000 assets = 2 million)\n",
    "To avoid this, covert it to $\\textbf{Q}^T\\textbf{Q}$, by the following:\n",
    "$\\textbf{BFB}^T$ = $\\textbf{BGGB}^T$ = $\\textbf{Q}^T\\textbf{Q}$, where $\\textbf{Q} = \\textbf{GB}^T$. Then the resulting $\\textbf{Qh} = \\textbf{R}$, and $\\textbf{Q}^T\\textbf{h}^T = \\textbf{R}^T$ are just K by 1 vector and 1 by K vector, respectively.\n",
    "\n",
    "$\\kappa$ is risk aversion factor, applied to any size metrics. (e.g. $GMV = \\sum_i^{N}(|\\mathbf{h}_i|)$ is a L1-norm)\n",
    "\n",
    "This model include aspects of Market neutral, Position Size, Portfolio Diversification without using Constraints, which is high in computational cost.\n",
    "This will be considered as the next step.\n",
    "\n",
    "### Risk Factors\n",
    "Rather than PCA, using commertial factor models are more realistic such as MSCI Barra. To be considered as the next step when using more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
